{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9718b09b",
   "metadata": {},
   "source": [
    "# Simulating the TechBio Learning Loop\n",
    "## From One-Off Experiments to a Learning System\n",
    "\n",
    "### 1. Introduction: Closing the Loop\n",
    "\n",
    "In the previous notebooks, we built the components piece by piece:\n",
    "* **Notebook 1 (Data Pipeline):** We cleaned the raw input.\n",
    "* **Notebook 2 (Automation):** We scaled the cleaning process.\n",
    "* **Notebook 3 (Flywheel):** We modeled the economics of learning.\n",
    "* **Notebook 4 (First Model):** We trained a single predictor on static data.\n",
    "\n",
    "Now, we connect them all into a **closed loop**.\n",
    "\n",
    "The goal of this notebook is not to learn machine learning in depth. The goal is to *see* how a TechBio system behaves across **multiple cycles**:\n",
    "\n",
    "> Assay → Data → Model → **Model-Guided Experiment** → New Data → Better Model\n",
    "\n",
    "We will simulate a simplified version of this process using **synthetic data**:\n",
    "1.  Start with a random experiment (Round 0).\n",
    "2.  Train a model on that data.\n",
    "3.  Use the model to *choose* the best candidates for the next experiment.\n",
    "4.  \"Run\" that experiment (reveal the true values).\n",
    "5.  Retrain and repeat.\n",
    "\n",
    "This shows how a TechBio platform gets smarter over time, finding better molecules faster than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d049498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup: Imports and Random Seed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c6b83",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Simulating a Hidden Biological Landscape\n",
    "\n",
    "To simulate a learning loop, we need an underlying “truth” that the model is trying to discover.\n",
    "\n",
    "Imagine we have a set of **candidates** (genes, small molecules, peptides, etc.).\n",
    "Each candidate has:\n",
    "* **Features:** (e.g., GC content, size, charge)\n",
    "* **True Response:** A hidden biological value (e.g., binding affinity or expression level).\n",
    "\n",
    "In real life, this \"True Response\" is unknown. In this simulation, we create it ourselves but hide it from the model, allowing us to check if the system actually learns the truth.\n",
    "\n",
    "We will:\n",
    "* Generate 200 synthetic candidates.\n",
    "* Assign two numerical features to each.\n",
    "* Compute a hidden **True Response** using a mathematical function.\n",
    "* Add noise later to mimic experimental error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af14f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic biological landscape (first 5 rows):\n",
      "              Feature_1  Feature_2  True_Response\n",
      "Candidate_ID                                     \n",
      "CAND_001       0.374540   0.642032       0.385772\n",
      "CAND_002       0.950714   0.084140       0.687954\n",
      "CAND_003       0.731994   0.161629       0.650229\n",
      "CAND_004       0.598658   0.898554       0.942709\n",
      "CAND_005       0.156019   0.606429       0.455326\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a synthetic \"biological space\" of candidates\n",
    "\n",
    "n_candidates = 200\n",
    "\n",
    "candidate_ids = [f\"CAND_{i:03d}\" for i in range(1, n_candidates + 1)]\n",
    "\n",
    "# Two simple features, e.g. physicochemical properties\n",
    "feature_1 = np.random.uniform(0, 1, size=n_candidates)   # e.g. normalized GC content\n",
    "feature_2 = np.random.uniform(0, 1, size=n_candidates)   # e.g. normalized length/charge/etc.\n",
    "\n",
    "# Hidden \"true\" response function (nonlinear combination)\n",
    "true_response = (\n",
    "    0.6 * feature_1\n",
    "    + 0.3 * feature_2\n",
    "    + 0.1 * np.sin(4 * np.pi * feature_1)\n",
    ")\n",
    "\n",
    "# Normalize to a nice 0–1 range\n",
    "true_response = (true_response - true_response.min()) / (true_response.max() - true_response.min())\n",
    "\n",
    "landscape_df = pd.DataFrame({\n",
    "    \"Candidate_ID\": candidate_ids,\n",
    "    \"Feature_1\": feature_1,\n",
    "    \"Feature_2\": feature_2,\n",
    "    \"True_Response\": true_response\n",
    "}).set_index(\"Candidate_ID\")\n",
    "\n",
    "print(\"Synthetic biological landscape (first 5 rows):\")\n",
    "print(landscape_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d5adc",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Simulating an Assay: Noisy Measurements\n",
    "\n",
    "In real biology, we never see the *true* response directly. Every experiment adds noise—from pipetting errors, instrument variability, or biological stochasticity.\n",
    "\n",
    "To keep our simulation realistic, we define an `run_assay` function that:\n",
    "1.  Looks up the hidden `True_Response`.\n",
    "2.  Adds Gaussian noise to mimic experimental error.\n",
    "3.  Returns a `Measured_Response`.\n",
    "\n",
    "This function acts as our **wet lab**. The model will only ever see the noisy measurements, but we will judge it based on how well it finds the true high-value candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b1ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Assay simulation: from true response to noisy measured response\n",
    "\n",
    "def run_assay(candidates: pd.Index, noise_std: float = 0.08):\n",
    "    \"\"\"\n",
    "    Simulate running a biological assay on a set of candidates.\n",
    "    Returns a DataFrame with noisy measured responses.\n",
    "    \"\"\"\n",
    "    subset = landscape_df.loc[candidates].copy()\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_std, size=len(subset))\n",
    "    subset[\"Measured_Response\"] = np.clip(subset[\"True_Response\"] + noise, 0.0, 1.0)\n",
    "    return subset[[\"Feature_1\", \"Feature_2\", \"True_Response\", \"Measured_Response\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e606c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Round 0: The Cold Start\n",
    "\n",
    "Before a TechBio platform becomes “smart,” it has to start with a blind guess. When there is no model yet (or no training data), we don't know which candidates are good.\n",
    "\n",
    "So, the very first experiment is usually a **random selection**.\n",
    "\n",
    "This first batch provides the initial \"ground truth\" data we need to train our first model.\n",
    "\n",
    "In this step, we will:\n",
    "* Pick 20 candidates at random.\n",
    "* Run our simulated assay on them.\n",
    "* Use these noisy measurements as our initial training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7bf5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: initial randomly measured candidates (first 5 rows):\n",
      "              Feature_1  Feature_2  True_Response  Measured_Response\n",
      "Candidate_ID                                                        \n",
      "CAND_104       0.508571   0.637430       0.646115           0.645984\n",
      "CAND_034       0.948886   0.492518       0.851964           0.817881\n",
      "CAND_189       0.529651   0.750615       0.745237           0.847274\n",
      "CAND_195       0.339030   0.340804       0.246355           0.239030\n",
      "CAND_139       0.363630   0.474174       0.309109           0.384075\n",
      "\n",
      "Number of measured candidates after Round 0: 20\n"
     ]
    }
   ],
   "source": [
    "# 4. Initial random experiment: Round 0\n",
    "\n",
    "n_initial = 20\n",
    "\n",
    "initial_candidates = np.random.choice(landscape_df.index, size=n_initial, replace=False)\n",
    "round0_df = run_assay(initial_candidates)\n",
    "\n",
    "print(\"Round 0: initial randomly measured candidates (first 5 rows):\")\n",
    "print(round0_df.head())\n",
    "\n",
    "print(f\"\\nNumber of measured candidates after Round 0: {len(round0_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a1971",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training the Engine\n",
    "\n",
    "Now we behave like a TechBio platform.\n",
    "\n",
    "We have a small table of measured candidates (from Round 0). We will train a **Random Forest Regressor** to predict the response based on the features.\n",
    "\n",
    "**Note:** Even though `True_Response` exists in our simulation, we pretend we don’t know it. We train only on `Measured_Response`, because that is all you would have in a real lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16cfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Helper: training and evaluation function\n",
    "\n",
    "def train_and_evaluate_model(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Train a RandomForestRegressor on train_df and evaluate on test_df.\n",
    "    We use Measured_Response as the target, but evaluate against True_Response\n",
    "    to see how close we get to the underlying biology.\n",
    "    \"\"\"\n",
    "    features = [\"Feature_1\", \"Feature_2\"]\n",
    "    \n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[\"Measured_Response\"]\n",
    "    \n",
    "    X_test = test_df[features]\n",
    "    y_test_true = test_df[\"True_Response\"]  # we use this only for evaluation in the simulation\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compare to the true underlying response\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_true, y_pred))\n",
    "\n",
    "    \n",
    "    return model, rmse, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892b655",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.1 Evaluating the First Model\n",
    "\n",
    "To measure success, we need to know if the model actually understands the landscape.\n",
    "\n",
    "We will:\n",
    "* Train on the measured candidates.\n",
    "* Predict values for all **unmeasured candidates**.\n",
    "* Compare those predictions to the hidden `True_Response`.\n",
    "\n",
    "We track **RMSE (Root Mean Squared Error)**. A lower RMSE means the model's mental map of the biology is getting closer to reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0941fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 model RMSE on unmeasured candidates: 0.110\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate the first model (Round 0)\n",
    "\n",
    "# Candidates not yet measured\n",
    "unmeasured_candidates = landscape_df.index.difference(round0_df.index)\n",
    "unmeasured_df = landscape_df.loc[unmeasured_candidates].copy()\n",
    "\n",
    "model_round0, rmse_round0, preds_round0 = train_and_evaluate_model(round0_df, unmeasured_df)\n",
    "\n",
    "print(f\"Round 0 model RMSE on unmeasured candidates: {rmse_round0:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242fd117",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Closing the Loop: Active Learning\n",
    "\n",
    "This is the definition of a TechBio platform. We don't just analyze data; we generate it.\n",
    "\n",
    "1.  The model is trained on the current measured data.\n",
    "2.  It predicts responses for **all unmeasured candidates**.\n",
    "3.  We select the **top K** candidates with the highest predicted response (Smart Selection).\n",
    "4.  We run the assay on those candidates (add them to our dataset).\n",
    "5.  We retrain the model.\n",
    "\n",
    "This creates the **Flywheel** we simulated in Notebook 3.\n",
    "\n",
    "We will repeat this loop several times and track:\n",
    "* **RMSE:** Does the model error go down?\n",
    "* **Candidate Quality:** Are we finding better molecules than if we just picked randomly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42906426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Loop parameters\n",
    "\n",
    "n_rounds = 5          # number of active learning rounds after Round 0\n",
    "k_per_round = 15      # how many new candidates we test each round\n",
    "\n",
    "# Tracking metrics\n",
    "rmse_history = []\n",
    "avg_true_selected_history = []\n",
    "avg_true_random_history = []\n",
    "\n",
    "# Initial state (Round 0 already measured)\n",
    "measured_df = round0_df.copy()\n",
    "current_unmeasured = landscape_df.index.difference(measured_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d4ecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: RMSE=0.110, avg true response (model-selected)=0.895, random=0.588\n",
      "Round 2: RMSE=0.115, avg true response (model-selected)=0.840, random=0.608\n",
      "Round 3: RMSE=0.116, avg true response (model-selected)=0.800, random=0.462\n",
      "Round 4: RMSE=0.118, avg true response (model-selected)=0.778, random=0.481\n",
      "Round 5: RMSE=0.119, avg true response (model-selected)=0.697, random=0.381\n"
     ]
    }
   ],
   "source": [
    "# 8. Run the learning loop\n",
    "\n",
    "for r in range(1, n_rounds + 1):\n",
    "    # Train model on currently measured data\n",
    "    unmeasured_df = landscape_df.loc[current_unmeasured].copy()\n",
    "    model, rmse, preds = train_and_evaluate_model(measured_df, unmeasured_df)\n",
    "    \n",
    "    rmse_history.append(rmse)\n",
    "    \n",
    "    # Add predictions to unmeasured_df\n",
    "    unmeasured_df[\"Predicted_Response\"] = preds\n",
    "    \n",
    "    # 1) Model-guided selection: top K by predicted response\n",
    "    selected_candidates = (\n",
    "        unmeasured_df\n",
    "        .sort_values(\"Predicted_Response\", ascending=False)\n",
    "        .head(k_per_round)\n",
    "        .index\n",
    "    )\n",
    "    \n",
    "    # 2) Baseline: random selection of K candidates (for comparison)\n",
    "    random_candidates = np.random.choice(current_unmeasured, size=k_per_round, replace=False)\n",
    "    \n",
    "    # True response averages for analysis\n",
    "    avg_true_selected = landscape_df.loc[selected_candidates, \"True_Response\"].mean()\n",
    "    avg_true_random = landscape_df.loc[random_candidates, \"True_Response\"].mean()\n",
    "    \n",
    "    avg_true_selected_history.append(avg_true_selected)\n",
    "    avg_true_random_history.append(avg_true_random)\n",
    "    \n",
    "    # Now \"run the assay\" on the model-selected candidates\n",
    "    new_measurements = run_assay(selected_candidates)\n",
    "    \n",
    "    # Add them to the measured_df\n",
    "    measured_df = pd.concat([measured_df, new_measurements])\n",
    "    \n",
    "    # Update unmeasured set\n",
    "    current_unmeasured = landscape_df.index.difference(measured_df.index)\n",
    "    \n",
    "    print(f\"Round {r}: RMSE={rmse:.3f}, avg true response (model-selected)={avg_true_selected:.3f}, random={avg_true_random:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca133c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "This notebook demonstrated the core mechanic of a TechBio company: **Iterative Improvement**.\n",
    "\n",
    "### 1. Models learn from partial data\n",
    "We started with zero knowledge. By Round 1, the model was already better than random guessing.\n",
    "\n",
    "### 2. Smart Selection wins\n",
    "Across the rounds, the model consistently picked candidates with higher true responses than a random picker would have. This is the \"efficiency\" gain of TechBio.\n",
    "\n",
    "### 3. The Loop is the Product\n",
    "The value isn't just the final drug candidate; it's the system that got smarter with every experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b81a85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Your Turn: Experiments to Try\n",
    "\n",
    "To understand this loop more deeply, try adjusting the simulation in small ways.\n",
    "Each change teaches something about how a real TechBio platform behaves.\n",
    "\n",
    "### **1. Change the number of rounds**\n",
    "Try:\n",
    "- `n_rounds = 3`\n",
    "- `n_rounds = 10`\n",
    "\n",
    "Watch how the RMSE curve changes. Does it plateau?\n",
    "\n",
    "### **2. Change how many candidates the model chooses per round**\n",
    "Raise or lower:\n",
    "- `k_per_round = 5`\n",
    "- `k_per_round = 20`\n",
    "\n",
    "This shows how **batch size** affects learning speed. Smaller batches are more efficient per candidate but take longer to cover the space.\n",
    "\n",
    "### **3. Change the noise level of the assay**\n",
    "In `run_assay()`, modify the noise parameter:\n",
    "```python\n",
    "# Low noise (High Quality Data)\n",
    "noise_std = 0.03\n",
    "\n",
    "# High noise (Messy Data)\n",
    "noise_std = 0.15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
