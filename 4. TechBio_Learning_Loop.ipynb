{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9718b09b",
   "metadata": {},
   "source": [
    "# Simulating the TechBio Learning Loop  \n",
    "From One-Off Experiments to a Learning System\n",
    "\n",
    "In the previous notebooks, we looked at the TechBio workflow piece by piece:\n",
    "\n",
    "1. **TechBio Flywheel** – the big picture: Biology → Data → Models → Validation → Impact  \n",
    "2. **TechBio Data Pipeline** – cleaning messy lab output into model-ready tables  \n",
    "3. **TechBio First Model** – training a simple model on clean biological data\n",
    "\n",
    "In this notebook, we connect those ideas and turn them into a **loop**.\n",
    "\n",
    "The goal is not to learn machine learning in depth.  \n",
    "The goal is to *see* how a TechBio system behaves across **multiple cycles**:\n",
    "\n",
    "> Assay → Data → Model → Model-guided Experiment → New Data → Better Model\n",
    "\n",
    "We will simulate a very simplified version of this process using **synthetic data**:\n",
    "\n",
    "- a fake \"biological assay\" that measures how well each candidate (e.g. gene, molecule) responds  \n",
    "- a model that tries to predict which candidates will have high response  \n",
    "- a loop where the model’s predictions guide which candidates we measure next  \n",
    "- repeated updates of the model as new data comes in\n",
    "\n",
    "This shows how a TechBio platform gets smarter over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d049498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup: Imports and Random Seed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c6b83",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1. Simulating a Hidden Biological Landscape\n",
    "\n",
    "To simulate a learning loop, we need an underlying “truth” that the model is trying to discover.\n",
    "\n",
    "Imagine we have a set of **candidates** (these could be genes, small molecules, peptides, etc.).  \n",
    "Each candidate has:\n",
    "\n",
    "- some **features** (e.g. GC content, size, charge, etc.)  \n",
    "- a **true underlying response** in a biological assay (e.g. how strongly it activates a pathway)  \n",
    "\n",
    "In real life, we don’t know the true function.  \n",
    "In this simulation, we create it ourselves and hide it from the model, so we can check how well the system learns.\n",
    "\n",
    "We will:\n",
    "\n",
    "- generate 200 synthetic candidates  \n",
    "- give each candidate two numerical features  \n",
    "- compute a **true_response** using a hidden function  \n",
    "- add noise later when we “measure” them, to mimic experimental error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af14f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic biological landscape (first 5 rows):\n",
      "              Feature_1  Feature_2  True_Response\n",
      "Candidate_ID                                     \n",
      "CAND_001       0.374540   0.642032       0.385772\n",
      "CAND_002       0.950714   0.084140       0.687954\n",
      "CAND_003       0.731994   0.161629       0.650229\n",
      "CAND_004       0.598658   0.898554       0.942709\n",
      "CAND_005       0.156019   0.606429       0.455326\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a synthetic \"biological space\" of candidates\n",
    "\n",
    "n_candidates = 200\n",
    "\n",
    "candidate_ids = [f\"CAND_{i:03d}\" for i in range(1, n_candidates + 1)]\n",
    "\n",
    "# Two simple features, e.g. physicochemical properties\n",
    "feature_1 = np.random.uniform(0, 1, size=n_candidates)   # e.g. normalized GC content\n",
    "feature_2 = np.random.uniform(0, 1, size=n_candidates)   # e.g. normalized length/charge/etc.\n",
    "\n",
    "# Hidden \"true\" response function (nonlinear combination)\n",
    "true_response = (\n",
    "    0.6 * feature_1\n",
    "    + 0.3 * feature_2\n",
    "    + 0.1 * np.sin(4 * np.pi * feature_1)\n",
    ")\n",
    "\n",
    "# Normalize to a nice 0–1 range\n",
    "true_response = (true_response - true_response.min()) / (true_response.max() - true_response.min())\n",
    "\n",
    "landscape_df = pd.DataFrame({\n",
    "    \"Candidate_ID\": candidate_ids,\n",
    "    \"Feature_1\": feature_1,\n",
    "    \"Feature_2\": feature_2,\n",
    "    \"True_Response\": true_response\n",
    "}).set_index(\"Candidate_ID\")\n",
    "\n",
    "print(\"Synthetic biological landscape (first 5 rows):\")\n",
    "print(landscape_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d5adc",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Simulating an Assay: Noisy Measurements\n",
    "\n",
    "In real biology, we never see the *true* response of a gene or molecule directly.  \n",
    "Every experiment adds a bit of noise — from the instrument, the protocol, or the environment.\n",
    "\n",
    "To keep our simulation realistic, we build a tiny artificial “assay”:\n",
    "\n",
    "- we start from the hidden `True_Response`  \n",
    "- we add a bit of Gaussian noise to mimic experimental error  \n",
    "- we store the result as `Measured_Response`\n",
    "\n",
    "This function acts as our **simulated biological experiment**.  \n",
    "The model will rely on these noisy measurements as it decides which candidates to test next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b1ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Assay simulation: from true response to noisy measured response\n",
    "\n",
    "def run_assay(candidates: pd.Index, noise_std: float = 0.08):\n",
    "    \"\"\"\n",
    "    Simulate running a biological assay on a set of candidates.\n",
    "    Returns a DataFrame with noisy measured responses.\n",
    "    \"\"\"\n",
    "    subset = landscape_df.loc[candidates].copy()\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_std, size=len(subset))\n",
    "    subset[\"Measured_Response\"] = np.clip(subset[\"True_Response\"] + noise, 0.0, 1.0)\n",
    "    return subset[[\"Feature_1\", \"Feature_2\", \"True_Response\", \"Measured_Response\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e606c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Round 0: A Random First Experiment\n",
    "\n",
    "Before any TechBio platform becomes “smart,” it has to start with a blind guess.  \n",
    "When there is no model yet, we don’t know which candidates are good or bad,  \n",
    "so the very first experiment is usually just a random selection.\n",
    "\n",
    "This first batch gives us the initial data we need to train a model.\n",
    "\n",
    "In this simulation, we will:\n",
    "\n",
    "- pick 20 candidates at random  \n",
    "- run our simulated assay on them  \n",
    "- use these noisy measurements as the **very first training dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7bf5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: initial randomly measured candidates (first 5 rows):\n",
      "              Feature_1  Feature_2  True_Response  Measured_Response\n",
      "Candidate_ID                                                        \n",
      "CAND_104       0.508571   0.637430       0.646115           0.645984\n",
      "CAND_034       0.948886   0.492518       0.851964           0.817881\n",
      "CAND_189       0.529651   0.750615       0.745237           0.847274\n",
      "CAND_195       0.339030   0.340804       0.246355           0.239030\n",
      "CAND_139       0.363630   0.474174       0.309109           0.384075\n",
      "\n",
      "Number of measured candidates after Round 0: 20\n"
     ]
    }
   ],
   "source": [
    "# 4. Initial random experiment: Round 0\n",
    "\n",
    "n_initial = 20\n",
    "\n",
    "initial_candidates = np.random.choice(landscape_df.index, size=n_initial, replace=False)\n",
    "round0_df = run_assay(initial_candidates)\n",
    "\n",
    "print(\"Round 0: initial randomly measured candidates (first 5 rows):\")\n",
    "print(round0_df.head())\n",
    "\n",
    "print(f\"\\nNumber of measured candidates after Round 0: {len(round0_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a1971",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training the First Model on Round 0\n",
    "\n",
    "Now we behave like a TechBio platform:\n",
    "\n",
    "- we have a small table of measured candidates (`Feature_1`, `Feature_2`, `Measured_Response`)  \n",
    "- we train a model that tries to predict the underlying response based on these features  \n",
    "\n",
    "Even though `True_Response` exists in our simulation, we pretend we don’t know it.  \n",
    "We only use `Measured_Response` as the **training target**, because that is what we would have in a real assay.\n",
    "\n",
    "We will use a **RandomForestRegressor** to predict response as a continuous value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16cfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Helper: training and evaluation function\n",
    "\n",
    "def train_and_evaluate_model(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Train a RandomForestRegressor on train_df and evaluate on test_df.\n",
    "    We use Measured_Response as the target, but evaluate against True_Response\n",
    "    to see how close we get to the underlying biology.\n",
    "    \"\"\"\n",
    "    features = [\"Feature_1\", \"Feature_2\"]\n",
    "    \n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[\"Measured_Response\"]\n",
    "    \n",
    "    X_test = test_df[features]\n",
    "    y_test_true = test_df[\"True_Response\"]  # we use this only for evaluation in the simulation\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compare to the true underlying response\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_true, y_pred))\n",
    "\n",
    "    \n",
    "    return model, rmse, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892b655",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1 Evaluating the First Model\n",
    "\n",
    "To evaluate how well the model understands the biological landscape, we:\n",
    "\n",
    "- train on the measured candidates  \n",
    "- test on all the **unmeasured candidates**  \n",
    "- compare predictions to `True_Response` (which we only know in this simulation)\n",
    "\n",
    "We will track **RMSE (Root Mean Squared Error)** between predicted and true responses.  \n",
    "Lower RMSE means the model is closer to the true underlying biology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0941fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 model RMSE on unmeasured candidates: 0.110\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate the first model (Round 0)\n",
    "\n",
    "# Candidates not yet measured\n",
    "unmeasured_candidates = landscape_df.index.difference(round0_df.index)\n",
    "unmeasured_df = landscape_df.loc[unmeasured_candidates].copy()\n",
    "\n",
    "model_round0, rmse_round0, preds_round0 = train_and_evaluate_model(round0_df, unmeasured_df)\n",
    "\n",
    "print(f\"Round 0 model RMSE on unmeasured candidates: {rmse_round0:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242fd117",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Making the Loop: Model-Guided Experiments\n",
    "\n",
    "Now we use the model in a more “TechBio” way:\n",
    "\n",
    "1. The model is trained on the current measured data.  \n",
    "2. It predicts responses for **all unmeasured candidates**.  \n",
    "3. We select the **top K** candidates with the highest predicted response.  \n",
    "4. We run the assay on those candidates (i.e. we measure them).  \n",
    "5. We add the new measurements to our training data.  \n",
    "6. We retrain the model on the enlarged dataset.  \n",
    "\n",
    "This creates a loop:\n",
    "\n",
    "> **Data → Model → Model-guided Experiment → New Data → Better Model**\n",
    "\n",
    "We repeat this loop several times and track:\n",
    "\n",
    "- how the model’s error (RMSE) changes over rounds  \n",
    "- how good the **selected candidates** actually are compared to random choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42906426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Loop parameters\n",
    "\n",
    "n_rounds = 5          # number of active learning rounds after Round 0\n",
    "k_per_round = 15      # how many new candidates we test each round\n",
    "\n",
    "# Tracking metrics\n",
    "rmse_history = []\n",
    "avg_true_selected_history = []\n",
    "avg_true_random_history = []\n",
    "\n",
    "# Initial state (Round 0 already measured)\n",
    "measured_df = round0_df.copy()\n",
    "current_unmeasured = landscape_df.index.difference(measured_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d4ecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: RMSE=0.110, avg true response (model-selected)=0.895, random=0.588\n",
      "Round 2: RMSE=0.115, avg true response (model-selected)=0.840, random=0.608\n",
      "Round 3: RMSE=0.116, avg true response (model-selected)=0.800, random=0.462\n",
      "Round 4: RMSE=0.118, avg true response (model-selected)=0.778, random=0.481\n",
      "Round 5: RMSE=0.119, avg true response (model-selected)=0.697, random=0.381\n"
     ]
    }
   ],
   "source": [
    "# 8. Run the learning loop\n",
    "\n",
    "for r in range(1, n_rounds + 1):\n",
    "    # Train model on currently measured data\n",
    "    unmeasured_df = landscape_df.loc[current_unmeasured].copy()\n",
    "    model, rmse, preds = train_and_evaluate_model(measured_df, unmeasured_df)\n",
    "    \n",
    "    rmse_history.append(rmse)\n",
    "    \n",
    "    # Add predictions to unmeasured_df\n",
    "    unmeasured_df[\"Predicted_Response\"] = preds\n",
    "    \n",
    "    # 1) Model-guided selection: top K by predicted response\n",
    "    selected_candidates = (\n",
    "        unmeasured_df\n",
    "        .sort_values(\"Predicted_Response\", ascending=False)\n",
    "        .head(k_per_round)\n",
    "        .index\n",
    "    )\n",
    "    \n",
    "    # 2) Baseline: random selection of K candidates (for comparison)\n",
    "    random_candidates = np.random.choice(current_unmeasured, size=k_per_round, replace=False)\n",
    "    \n",
    "    # True response averages for analysis\n",
    "    avg_true_selected = landscape_df.loc[selected_candidates, \"True_Response\"].mean()\n",
    "    avg_true_random = landscape_df.loc[random_candidates, \"True_Response\"].mean()\n",
    "    \n",
    "    avg_true_selected_history.append(avg_true_selected)\n",
    "    avg_true_random_history.append(avg_true_random)\n",
    "    \n",
    "    # Now \"run the assay\" on the model-selected candidates\n",
    "    new_measurements = run_assay(selected_candidates)\n",
    "    \n",
    "    # Add them to the measured_df\n",
    "    measured_df = pd.concat([measured_df, new_measurements])\n",
    "    \n",
    "    # Update unmeasured set\n",
    "    current_unmeasured = landscape_df.index.difference(measured_df.index)\n",
    "    \n",
    "    print(f\"Round {r}: RMSE={rmse:.3f}, avg true response (model-selected)={avg_true_selected:.3f}, random={avg_true_random:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca133c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Points and Summary\n",
    "\n",
    "This notebook showed how a TechBio system becomes smarter through **iteration**, not by running a single model once.  \n",
    "We started with a hidden biological landscape, ran a noisy assay, trained a model, and then let that model guide the next round of experiments.\n",
    "\n",
    "Here are the essential ideas:\n",
    "\n",
    "### ● 1. Real biology always begins with partial information  \n",
    "The first experiment is usually random.  \n",
    "You don’t know which candidates are good, so you measure a small batch just to get started.\n",
    "\n",
    "### ● 2. Models learn from whatever data exists  \n",
    "Our first model was trained on noisy, limited measurements — just like in real labs.  \n",
    "Even a weak first model is enough to guide the next step.\n",
    "\n",
    "### ● 3. Model-guided experiments are more informative than random ones  \n",
    "Across rounds, the model consistently selected candidates with **higher true responses** than random selection.  \n",
    "This is the core advantage of a TechBio platform.\n",
    "\n",
    "### ● 4. The loop improves the system  \n",
    "Each round adds new measurements → expands the dataset → sharpens the model.  \n",
    "This is the engine behind TechBio companies.\n",
    "\n",
    "### ● 5. Noise, limited data, and uncertainty are not bugs — they are reality  \n",
    "Our noisy assay reflects the variability of real biological experiments.  \n",
    "Despite noise, the loop still converges toward better predictions.\n",
    "\n",
    "### ● 6. The goal here is conceptual, not algorithmic  \n",
    "This notebook is not about learning machine learning deeply.  \n",
    "It exists to make the **feedback loop** real and visible, connecting directly to the book’s ideas.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Your Turn: Experiments to Try\n",
    "\n",
    "To understand this loop more deeply, try adjusting the simulation in small ways.  \n",
    "Each change teaches something about how a real TechBio platform behaves.\n",
    "\n",
    "### **1. Change the number of rounds**\n",
    "Try:\n",
    "- `n_rounds = 3`\n",
    "- `n_rounds = 10`\n",
    "\n",
    "Watch how the RMSE curve changes.\n",
    "\n",
    "### **2. Change how many candidates the model chooses per round**\n",
    "Raise or lower:\n",
    "- `k_per_round = 5`\n",
    "- `k_per_round = 20`\n",
    "\n",
    "This shows how batch size affects learning speed.\n",
    "\n",
    "### **3. Change the noise level of the assay**\n",
    "In `run_assay()`, modify:\n",
    "```python\n",
    "noise_std=0.03\n",
    "noise_std=0.15\n",
    "\n",
    "### High noise makes learning slower; low noise makes it faster\n",
    "\n",
    "Increasing the assay noise makes the model’s job harder, because every measurement becomes less reliable.  \n",
    "Decreasing the noise produces cleaner signals and faster learning.  \n",
    "Changing this single parameter shows how fragile or stable the loop is when experiments become noisy.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Change the hidden biological function\n",
    "\n",
    "Modify how `True_Response` is generated.  \n",
    "For example:\n",
    "\n",
    "- remove the sine term  \n",
    "- add a squared term like `feature_1**2`  \n",
    "- change the weights (e.g. make Feature_2 more important)  \n",
    "\n",
    "Different underlying functions create easier or harder landscapes for the model to learn.  \n",
    "This helps you see how the difficulty of the biology affects the behavior of the whole loop.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Try a different regression model\n",
    "\n",
    "Swap out the Random Forest for another simple regressor:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "or\n",
    "from sklearn.neighbors import KNeighborsRegressor```\n",
    "\n",
    "Compare the RMSE curves and the quality of the candidates selected in each round.\n",
    "Each model has its own strengths and weaknesses, and changing it shows how model choice affects the stability and speed of the learning loop.\n",
    "\n",
    "---\n",
    "\n",
    "6. Track another metric\n",
    "\n",
    "To make the simulation feel more like a real TechBio platform, try logging additional metrics such as:\n",
    "\n",
    "* the best true response discovered so far\n",
    "* the median true response of all measured candidates\n",
    "* the total number of candidates measured across rounds\n",
    "\n",
    "These resemble the simple dashboards that real TechBio systems use to monitor platform progress.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Takeaway**\n",
    "\n",
    "A TechBio system is not simply “a model running on data.”\n",
    "It is a loop, where:\n",
    "\n",
    "* experiments create data\n",
    "* data trains models\n",
    "* models guide new experiments\n",
    "* new experiments create better data\n",
    "\n",
    "Every turn of this loop improves the system’s knowledge and performance.\n",
    "\n",
    "This notebook gives you a small, synthetic demonstration of that process —\n",
    "just enough to show how the ideas in the book translate into a practical workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
